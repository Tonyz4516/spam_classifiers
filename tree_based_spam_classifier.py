# -*- coding: utf-8 -*-
"""2019.06_tree_based_spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DgOk8ka_16KXbzrw91CNoKfIyowTxw78
"""

# import
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
import string
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

# models
from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,AdaBoostClassifier

! wget -nc -P tree_spam/ https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip
! unzip -n tree_spam/smsspamcollection.zip -d tree_spam/

# load the data
with open("tree_spam/SMSSpamCollection") as f:  # don't use a for loop here
    content = f.readlines()
content = [x.strip() for x in content]

# make a matrix of n * 2: first column is ham/spam, second is the msg
list_of_list = []
for msg in content:
    list_of_list.append(msg.split("\t"))

# convert to numpy's ndarray, and then pandas
spam_data = np.asarray(list_of_list)

# convert to pandas dataframe
spam_df = pd.DataFrame(data = spam_data[0:,0:])

# change col names
spam_df.columns = ["class", "text"]

# build function tokenizer
def process_text(text):
    """
    1. remove puntuation
    2. remove stopwords
    3. return list
    """
    #1:
    punc_removed = [char for char in text if char not in string.punctuation]
    punc_removed = "".join(punc_removed)
    
    #2:
    clean_words = [word for word in punc_removed.split() if word not in\
                  stopwords.words("english")]
    
    #3:
    return clean_words

# Commented out IPython magic to ensure Python compatibility.
# %%time
# spam_df["processed"] = spam_df.iloc[:,1].apply(process_text)

# vectorize it to Document-Term Matrix
vectorizer = TfidfVectorizer()
# convert to array for vectorizer to proceed
msg = ["".join(text) for text in spam_df["processed"].values]
msg = vectorizer.fit_transform(msg)

# train_test split
msg_train, msg_test, class_train, class_test = \
train_test_split(msg, spam_df["class"],\
                 test_size = 0.2, random_state = 1234)

"""
Three models will be applied here:
1. BaggingClassifier
2. RandomForestClassifier
3. AdaBoostClassifier
"""

model0 = BaggingClassifier()
model1 = RandomForestClassifier()
model2 = AdaBoostClassifier()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 0. bagging
# model0.fit(msg_train, class_train)

# accuracy
print(model0.score(msg_test, class_test))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 1. random forest
# model1.fit(msg_train, class_train)

# accuracy
print(model1.score(msg_test, class_test))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # 2. adaboost
# model2.fit(msg_train, class_train)

# accuracy
print(model2.score(msg_test, class_test))

"""
show the accuracy
"""

import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from IPython.core.display import display, HTML

trace = go.Heatmap(z = confusion_matrix(class_test,model0.predict(msg_test)),\
    x = ["ham","spam"], y = ["ham","spam"])
data=[trace]

layout = go.Layout(
    title=go.layout.Title(
        text='model 0: bagging'
    )
)

fig = go.Figure(data=data, layout=layout)
fig.show()

trace = go.Heatmap(z = confusion_matrix(class_test,model1.predict(msg_test)),\
    x = ["ham","spam"], y = ["ham","spam"])
data=[trace]

layout = go.Layout(
    title=go.layout.Title(
        text='model 1: random forest'
    )
)

fig = go.Figure(data=data, layout=layout)
fig.show()

trace = go.Heatmap(z = confusion_matrix(class_test,model2.predict(msg_test)),\
    x = ["ham","spam"], y = ["ham","spam"])
data=[trace]

layout = go.Layout(
    title=go.layout.Title(
        text='model 2: adaboost'
    )
)

fig = go.Figure(data=data, layout=layout)
fig.show()