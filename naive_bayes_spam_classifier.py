# -*- coding: utf-8 -*-
"""2019.06_naive_bayes_spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A3g5BpRpGzWndmF3YX5dRcLvtv4U_4OH
"""

# import
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
import string
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report,confusion_matrix

# wget -nc checks if file already downloaded
# wget -P specify folder
! wget -nc -P naive_bayes_spam/ https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip
! unzip -n naive_bayes_spam/smsspamcollection.zip -d naive_bayes_spam/

# load the data
with open("naive_bayes_spam/SMSSpamCollection") as f:  # don't use a for loop here
    content = f.readlines()
content = [x.strip() for x in content]

# make a matrix of n * 2: first column is ham/spam, second is the msg
list_of_list = []
for msg in content:
    list_of_list.append(msg.split("\t"))

# convert to numpy's ndarray
spam_data = np.asarray(list_of_list)

# convert to pandas dataframe
spam_df = pd.DataFrame(data = spam_data[0:,0:])

# change col names
spam_df.columns = ["class", "text"]

"""1. tokenize all words and filter stopwords
2. use naive bayes classifier on each words
"""

# build function tokenizer
def process_text(text):
    """
    1. remove puntuation
    2. remove stopwords
    3. return list
    """
    #1:
    punc_removed = [char for char in text if char not in string.punctuation]
    punc_removed = "".join(punc_removed)
    
    #2:
    clean_words = [word for word in punc_removed.split() if word not in\
                  stopwords.words("english")]
    
    #3:
    return clean_words

# Commented out IPython magic to ensure Python compatibility.
# %%time
# spam_df["processed"] = spam_df.iloc[:,1].apply(process_text)

# vectorize it to Document-Term Matrix
vectorizer = TfidfVectorizer()
# convert to array for vectorizer to proceed
msg = ["".join(text) for text in spam_df["processed"].values]
msg = vectorizer.fit_transform(msg)

# train_test split
msg_train, msg_test, class_train, class_test = \
train_test_split(msg, spam_df["class"],\
                 test_size = 0.2)
# , random_state = 1234

# fit the model and predict
model = MultinomialNB()
model.fit(msg_train, class_train)

prediction = model.predict(msg_test)

"""
calculate the accuracy
"""
import plotly.graph_objs as go

data = go.Heatmap(z = confusion_matrix(class_test,prediction),\
    x = ["ham","spam"], y = ["ham","spam"])
fig = go.Figure(data)
fig.show()

from sklearn.metrics import accuracy_score
acc = accuracy_score(class_test,prediction)
print(acc)